---
topic: "Predictive Modeling in `R`"
---

```{r}
#| echo: false
#| warning: false
#| message: false
library(tidyverse)
library(palmerpenguins)
```

# {{< meta topic >}}

## Where are We?

::: {style="font-size: 85%;"}
- `R` Basics
- Data Visualization
- [`R` Programming]{color="blue"}
  - [Statistical Analysis]{color="blue"}
- Tidy Data Manipulation
- `python` Basics
- Web Scraping with `python`

:::


## Goals for this Video

In this video: 

::: incremental

- What is Predictive Modeling?
- The `tidymodels` Ecosystem

:::


## tidymodels

Strength of `R`: 
 
- Thousands of authors contributing packages to CRAN

. . . 

Weakness of `R`: 

- Thousands of authors contributing *slightly incompatible* packages to CRAN

. . . 

No two modeling packages have _exactly_ the same API. Makes 
changing between interfaces cumbersome

## tidymodels

`tidymodels` attemps to provide a _uniform_ interface
to a wide variety of _predictive_ Machine Learning tools

Advantages: 

- Easy to swap out different algorithms to find the best

Disadvantages: 

- Harder to take advantage of the strengths of each approach

## ML vs Statistical Pipelines

Statistics / Data Science: 

- Find the model that _fits_ the data best
- Model should capture all important data features
- _Interpretability_ 
- History: Grounded in lab sciences where experiments are
  expensive and data is limited 
  
## ML vs Statistical Pipelines

Machine Learning: 

- Find the model that _predicts_ the data best
- No "perfect" model - just the best one we've found so far
- Black-box techniques are great, _if effective_
- History: Silicon Valley "at scale"

Validation based on _of-of-sample_ or _test_ predictions

## Validating Predictive Power

How to check whether a model _predicts_ well?

. . . 

Need more data! But where to get more data? 

- Actually get more data (hard, expensive, slow)
- Split data into parts - test/training split
- Cross-Validation
- Resampling

. . . 

Today, we'll primarily use a combination: **Test/Train** split & **Cross-Validation**!

## Cross-Validation

![](https://scikit-learn.org/1.5/_images/grid_search_cross_validation.png)

Cross-Validation is done on the *estimator*, not the fitted algorithm

## tidymodels

`tidymodels` workflow: 

- Initial Split
- Pre-Process
- Fit (*many*) models
- Select best
- Refit
- Test Set Assessment

`tidymodels` is _very_ punny, so a bit hard to tell which step is which...

## Acquire Data

```{r}
#| echo: true
library(tidymodels); library(readr)

hotels <- 
  read_csv("https://tidymodels.org/start/case-study/hotels.csv") |>
  mutate(across(where(is.character), as.factor))

glimpse(hotels)
```

## Initial Split

```{r}
#| echo: true
#| eval: true
# Stratified sampling to ensure balance
splits      <- initial_split(hotels, 
                             strata = children)

hotel_train <- training(splits)
hotel_test  <- testing(splits)

hotel_train
```

## Pre-Process {.scrollable}

```{r}
library(recipes)
```

```{r}
#| eval: true
#| echo: true
#| message: true
holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter", 
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

lr_recipe <- 
  recipe(children ~ ., data = hotels) |> 
  step_date(arrival_date) |> 
  step_holiday(arrival_date, holidays = holidays) |> 
  step_rm(arrival_date) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

lr_recipe
```

## Fit Models

```{r}
#| eval: true
#| echo: true
lr_model <- 
  logistic_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet")

lr_model
```

## Select Best

Find a _grid_ of parameters 

```{r}
#| eval: true
#| echo: true
lr_reg_grid <- data.frame(penalty = 10^seq(-4, -1, length.out = 30))
```

Perform CV splits: 

```{r}
#| eval: true
#| echo: true
lr_folds <- vfold_cv(hotel_train, v = 5)
```

## Select Best

Define a *workflow*: 

```{r}
#| eval: true
#| echo: true
lr_workflow <-  
  workflow() |> 
  add_model(lr_model) |> 
  add_recipe(lr_recipe)
```

Fit workflow to a grid of parameters: 

```{r}
#| eval: true
#| echo: true
#| cache: true
lr_results <- 
  lr_workflow |> 
  tune_grid(grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE, 
                                   save_workflow=TRUE),
            resamples = lr_folds,
            metrics = metric_set(roc_auc))
```

## Select Best

Visual examination

```{r}
#| eval: true
#| echo: true
lr_results |> 
  collect_metrics() |> 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())
```

## Select Best

```{r}
#| eval: true
#| echo: true
lr_results |> show_best()
lr_best <- lr_results |> select_best()
lr_best
```


## Refit Best Model

```{r}
#| eval: true
#| echo: true
lr_best_fit <- lr_results |> fit_best()
lr_best_fit
```

## Test Set Assessment

```{r}
#| eval: true
#| echo: true
predict(lr_best_fit, hotel_test)
```

```{r}
#| echo: true
#| eval: true
augment(lr_best_fit, hotel_test)
```


## tidymodels tools

- Feature Importance Scores
- Model Stacking
- Probabilistic Predictions
- Uncertainty Bounds (Conformal Inference)
- Multilevel (Mixed-Effect) Models
- Fairness Audits


## Looking Ahead

Applications to course project: 

::: {.incremental}

- Two variables are *statistically independent* if
  one cannot be used to predict the other
- Many questions take the form "Is $X$ related to $Y$?"
- Build a *model* to predict $Y$ from $X$
  - If model doesn't use $X$ (small coefficient or importance), unlikely to be related
  - Move beyond simple linear correlation
- Also consider *occlusion* studies

:::

## Learning More

For more: 

- Modeling - future STA and CIS classes
- `tidymodels` - [Case Study](https://www.tidymodels.org/start/case-study/)


. . . 

To see more worked examples of predictive models, check out the 
[`tidymodels` Gallery](https://www.tidymodels.org/learn/)


