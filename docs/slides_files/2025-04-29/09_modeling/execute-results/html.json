{
  "hash": "7043c64a676a23b667de7e5ec3c0b7db",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntopic: \"Basic Statistical Analysis\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n# {{< meta topic >}}\n\n## Where are We?\n\n::: {style=\"font-size: 85%;\"}\n- `R` Basics\n- Data Visualization\n- [`R` Programming]{color=\"blue\"}\n  - [Statistical Analysis]{color=\"blue\"}\n- Tidy Data Manipulation\n- `python` Basics\n- Web Scraping with `python`\n\n:::\n\n\n## Goals for this Video\n\nIn this video: \n\n::: incremental\n\n- Formula Syntax\n- Classical Statistical Models with `R`\n- Key Accessor Functions\n- Predictive Modeling in `R`\n\n:::\n\n\n## Formula Notation\n\n`R` was designed for statistical analysis (originally called `S`)\n\n. . . \n\nMajor contributions\n\n- `data.frame` / tidy structure\n- Formula language (\"Wilkinson-Rogers notation\")\n\n## Formula Notation\n\nIn `R`, a `formula` is a special object defined by a `~`\n\n. . . \n\nMost common structure\n\n> `y ~ x1 + x2`\n\nPredict variable `y` using `x1` and `x2`\n\n. . . \n\n- Modern `R` uses formulas in many other contexts\n- Various extensions provided by packages\n\n## Modeling Functions\n\nBasic modeling function: `lm` (_linear model_)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\nlm(body_mass_g ~ flipper_length_mm, data=penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm, data = penguins)\n\nCoefficients:\n      (Intercept)  flipper_length_mm  \n         -5780.83              49.69  \n```\n\n\n:::\n:::\n\n\n\n\n\n## Modeling Functions\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(body_mass_g ~ flipper_length_mm, data=penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm, data = penguins)\n\nCoefficients:\n      (Intercept)  flipper_length_mm  \n         -5780.83              49.69  \n```\n\n\n:::\n:::\n\n\n\n\n- Provide model (`formula`) and data (`data.frame`) instead of $X, y$\n- By default automatically includes an intercept term\n\n## Modeling Functions\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(body_mass_g ~ flipper_length_mm + species, data=penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + species, data = penguins)\n\nCoefficients:\n      (Intercept)  flipper_length_mm   speciesChinstrap      speciesGentoo  \n         -4031.48              40.71            -206.51             266.81  \n```\n\n\n:::\n:::\n\n\n\n\nAutomatically: \n\n- Encodes categorical (`factor`) variables\n  - `?C` for details\n- Removes extra / redundant columns\n\n\n## Modeling Functions\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(body_mass_g ~ flipper_length_mm*bill_depth_mm, data=penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm * bill_depth_mm, \n    data = penguins)\n\nCoefficients:\n                    (Intercept)                flipper_length_mm  \n                     -36097.064                          196.074  \n                  bill_depth_mm  flipper_length_mm:bill_depth_mm  \n                       1771.796                           -8.596  \n```\n\n\n:::\n:::\n\n\n\n\n- `*` creates both 'main' effects and interactions\n\n\n## Modeling Functions\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(body_mass_g ~ flipper_length_mm*species, data=penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm * species, data = penguins)\n\nCoefficients:\n                       (Intercept)                   flipper_length_mm  \n                         -2535.837                              32.832  \n                  speciesChinstrap                       speciesGentoo  \n                          -501.359                           -4251.444  \nflipper_length_mm:speciesChinstrap     flipper_length_mm:speciesGentoo  \n                             1.742                              21.791  \n```\n\n\n:::\n:::\n\n\n\n\n- `*` of continuous and categorical creates ANCOVA\n\n## Modeling Functions \n\nMany extensions\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mgcv)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: nlme\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'nlme'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    collapse\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is mgcv 1.9-3. For overview type 'help(\"mgcv-package\")'.\n```\n\n\n:::\n\n```{.r .cell-code}\ngam(body_mass_g ~ s(flipper_length_mm) + s(bill_depth_mm, by=species), data=penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nbody_mass_g ~ s(flipper_length_mm) + s(bill_depth_mm, by = species)\n\nEstimated degrees of freedom:\n1.56 1.96 1.00 4.39  total = 9.91 \n\nGCV score: 106441.7     \n```\n\n\n:::\n:::\n\n\n\nFits a *mixed-effect non-linear regression*\n\n## Accessors\n\nHelper functions to access fitted models: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(body_mass_g ~ flipper_length_mm*bill_depth_mm + species, data=penguins)\ncoef(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    (Intercept)               flipper_length_mm \n                  -4844.4526376                      27.3153209 \n                  bill_depth_mm                speciesChinstrap \n                    200.0652179                    -131.5598847 \n                  speciesGentoo flipper_length_mm:bill_depth_mm \n                   1283.9149548                      -0.0900381 \n```\n\n\n:::\n:::\n\n\n\n\n## Accessors\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm * bill_depth_mm + \n    species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-900.40 -238.12  -40.12  228.86 1085.32 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                     -4.844e+03  5.298e+03  -0.914   0.3611    \nflipper_length_mm                2.732e+01  2.699e+01   1.012   0.3123    \nbill_depth_mm                    2.001e+02  2.945e+02   0.679   0.4973    \nspeciesChinstrap                -1.316e+02  5.192e+01  -2.534   0.0117 *  \nspeciesGentoo                    1.284e+03  1.572e+02   8.166 6.51e-15 ***\nflipper_length_mm:bill_depth_mm -9.004e-02  1.495e+00  -0.060   0.9520    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 331.3 on 336 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8319,\tAdjusted R-squared:  0.8294 \nF-statistic: 332.5 on 5 and 336 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Accessors\n\nIn-sample / training prediction: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        5        6        7        8        9 \n3536.088 3425.933 3767.175 3953.280 4114.393 3370.697 4059.188 3734.055 \n      10       11       12       13       14       15       16       17 \n4041.210 3370.937 3253.055 3359.674 4249.574 4409.196 3473.547 3949.683 \n      18       19       20       21       22       23       24       25 \n4310.849 3557.933 4380.573 3282.907 3510.456 3832.665 3528.570 3234.669 \n      26       27       28       29       30       31       32       33 \n3726.523 3568.992 3543.295 3286.946 3547.228 3091.116 3348.770 3550.685 \n      34       35       36       37       38       39       40       41 \n3649.683 3584.667 4358.365 4004.618 3473.685 3646.349 3686.382 3433.145 \n      42       43       44       45       46       47       48       49 \n3840.178 3627.582 4102.980 3308.480 3785.068 3616.823 3521.615 3620.406 \n      50       51       52       53       54       55       56       57 \n4249.574 3480.928 3752.137 3620.406 4168.735 3579.941 3774.117 3444.264 \n      58       59       60       61       62       63       64       65 \n3861.936 3150.175 3942.338 3308.480 4332.949 3326.821 3726.647 3319.386 \n      66       67       68       69       70       71       72       73 \n3690.091 3438.661 3788.765 3382.561 4099.392 3821.660 3711.885 3646.936 \n      74       75       76       77       78       79       80       81 \n3982.659 3547.223 3858.429 3444.955 3741.432 3213.485 3967.934 3466.569 \n      82       83       84       85       86       87       88       89 \n3719.903 3708.201 3971.549 3627.823 4161.456 3913.139 3722.836 3832.665 \n      90       91       92       93       94       95       96       97 \n3785.068 3947.038 4042.282 3345.161 3554.255 3433.358 4264.409 3748.477 \n      98       99      100      101      102      103      104      105 \n3884.079 2980.693 3781.480 3671.813 4336.307 3091.663 4004.618 3825.399 \n     106      107      108      109      110      111      112      113 \n3649.683 3724.236 4004.618 3223.682 4000.892 3570.903 4084.993 3660.980 \n     114      115      116      117      118      119      120      121 \n4092.056 4158.140 3847.595 3404.175 4325.323 3429.959 3722.836 3415.036 \n     122      123      124      125      126      127      128      129 \n4172.287 3094.759 4037.976 3150.955 4052.101 3591.249 3821.927 3499.815 \n     130      131      132      133      134      135      136      137 \n4152.595 3620.406 4037.358 3807.130 3961.028 3488.327 3547.223 3572.963 \n     138      139      140      141      142      143      144      145 \n4277.969 3235.117 3697.517 3551.367 3415.036 3129.468 3455.744 3470.758 \n     146      147      148      149      150      151      152      153 \n3638.615 3748.477 3557.933 3730.674 3734.055 3396.713 4012.327 4593.082 \n     154      155      156      157      158      159      160      161 \n5645.496 4729.996 5136.843 4932.508 4621.302 4846.576 5180.825 4577.078 \n     162      163      164      165      166      167      168      169 \n5095.145 4761.861 5247.505 4761.861 4898.577 4820.575 5201.160 4621.302 \n     170      171      172      173      174      175      176      177 \n5214.684 4776.450 5222.623 4974.450 4932.508 4880.489 5167.427 4679.518 \n     178      179      180      181      182      183      184      185 \n5040.932 4922.395 5022.862 4766.228 5206.763 5258.638 4722.075 4724.430 \n     186      187      188      189      190      191      192      193 \n5771.046 5116.634 5387.019 4735.779 5541.519 4587.236 4968.045 4605.370 \n     194      195      196      197      198      199      200      201 \n5462.316 4657.534 5048.826 5366.684 4876.212 4693.765 5444.335 4663.424 \n     202      203      204      205      206      207      208      209 \n5167.427 4748.112 4990.454 4784.344 5282.509 4966.475 5224.788 4641.637 \n     210      211      212      213      214      215      216      217 \n5152.685 4750.440 5310.513 4623.504 5160.634 4798.020 5563.784 4982.443 \n     218      219      220      221      222      223      224      225 \n5735.175 4888.419 5601.704 5008.480 5230.580 5048.826 5286.750 5286.750 \n     226      227      228      229      230      231      232      233 \n5038.686 5048.826 5591.689 4722.075 5387.019 4806.013 5482.561 4854.479 \n     234      235      236      237      238      239      240      241 \n5286.750 4872.577 5418.451 4727.795 5773.337 4992.494 4956.406 4763.990 \n     242      243      244      245      246      247      248      249 \n5771.046 5100.756 5737.430 4854.479 5454.431 4942.658 5434.276 5193.320 \n     250      251      252      253      254      255      256      257 \n5132.584 4602.212 5552.219 5126.721 5719.477 5113.215 5360.403 4832.086 \n     258      259      260      261      262      263      264      265 \n5221.639 4838.691 5270.998 4659.771 4885.198 5084.950 5547.870 5007.110 \n     266      267      268      269      270      271      273      274 \n5645.496 4912.317 5591.689 5201.160 5420.707 4761.861 4896.367 5330.669 \n     275      276      277      278      279      280      281      282 \n4908.772 5169.908 3540.254 3934.937 3803.451 3583.949 4015.194 3676.252 \n     283      284      285      286      287      288      289      290 \n3235.614 3723.470 3799.872 4058.951 3547.688 4029.896 3250.283 3807.980 \n     291      292      293      294      295      296      297      298 \n3342.480 4080.931 4051.660 3239.137 3616.917 3672.117 3147.253 3441.403 \n     299      300      301      302      303      304      305      306 \n3173.539 3839.989 3617.364 3869.332 3836.911 3946.146 3496.263 4255.776 \n     307      308      309      310      311      312      313      314 \n3173.539 4299.292 3191.862 3986.602 3745.120 3519.817 3690.368 4510.159 \n     315      316      317      318      319      320      321      322 \n3302.642 4237.616 4292.771 3338.444 3861.969 3478.892 3643.068 3880.767 \n     323      324      325      326      327      328      329      330 \n3488.846 4361.987 3558.318 3585.133 3446.958 3971.751 3456.344 4150.211 \n     331      332      333      334      335      336      337      338 \n3301.799 3832.867 3276.822 4186.569 3960.980 3865.558 4190.532 3206.875 \n     339      340      341      342      343      344 \n3453.107 4270.520 3833.665 3620.764 4202.192 3840.266 \n```\n\n\n:::\n:::\n\n\n\n\n## Accessors\n\nOut-of-sample / test prediction:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoppenguins <- penguins |> slice_max(body_mass_g, n=10)\npredict(m, newdata=toppenguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4        5        6        7        8 \n5214.684 5771.046 5387.019 5420.707 5482.561 5547.870 4898.577 5201.160 \n       9       10       11 \n5591.689 5601.704 5591.689 \n```\n\n\n:::\n:::\n\n\n\n\n## Accessors\n\nFor just `lm`: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmethods(class=\"lm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] add1           alias          anova          case.names     coerce        \n [6] confint        cooks.distance deviance       dfbeta         dfbetas       \n[11] drop1          dummy.coef     effects        extractAIC     family        \n[16] formula        fortify        hatvalues      influence      initialize    \n[21] kappa          labels         logLik         model.frame    model.matrix  \n[26] nobs           plot           predict        print          proj          \n[31] qqnorm         qr             residuals      rstandard      rstudent      \n[36] show           simulate       slotsFromS3    summary        variable.names\n[41] vcov          \nsee '?methods' for accessing help and source code\n```\n\n\n:::\n:::\n\n\n\n\nEven more for other models\n\n# New Material: Predictive Modeling in `tidymodels`\n\n## Agenda\n\n- Predictive Modeling with `tidymodels`\n\nAdapted from [Case Study](https://www.tidymodels.org/start/case-study/)\n\n## `tidymodels`\n\nStrength of `R`: \n \n- Thousands of authors contributing packages to CRAN\n\n. . . \n\nWeakness of `R`: \n\n- Thousands of authors contributing *slightly incompatible* packages to CRAN\n\n. . . \n\nNo two modeling packages have _exactly_ the same API. Makes \nchanging between interfaces cumbersome\n\n## `tidymodels`\n\n`tidymodels` attemps to provide a _uniform_ interface\nto a wide variety of _predictive_ Machine Learning tools\n\nAdvantages: \n\n- Easy to swap out different algorithms to find the best\n\nDisadvantages: \n\n- Harder to take advantage of the strengths of each approach\n\n. . . \n\nI have dedicated my academic life to the differences in these\nmethods, but 99% of the time, \"black-box\" prediction is good\nenough. In STA 9890, we get into the weeds - not here.\n\n## ML vs Statistical Pipelines\n\nStatistics / Data Science: \n\n- Find the model that _fits_ the data best\n- Model should capture all important data features\n- _Interpretability_ \n- History: Grounded in lab sciences where experiments are\n  expensive and data is limited \n  \n## ML vs Statistical Pipelines\n\nMachine Learning: \n\n- Find the model that _predicts_ the data best\n- No \"perfect\" model - just the best one we've found so far\n- Black-box techniques are great, _if effective_\n- History: Silicon Valley \"at scale\"\n\nValidation based on _of-of-sample_ or _test_ predictions\n\n## Validating Predictive Power\n\nHow to check whether a model _predicts_ well?\n\n. . . \n\nNeed more data! But where to get more data? \n\n- Actually get more data (hard, expensive, slow)\n- Split data into parts - test/training split\n- Cross-Validation\n- Resampling\n\n. . . \n\nToday, we'll primarily use a combination: **Test/Train** split & **Cross-Validation**!\n\n## Cross-Validation\n\n![](https://scikit-learn.org/1.5/_images/grid_search_cross_validation.png)\n\nCross-Validation is done on the *estimator*, not the fitted algorithm\n\n## `tidymodels`\n\n`tidymodels` workflow: \n\n- Initial Split\n- Pre-Process\n- Fit (*many*) models\n- Select best\n- Refit\n- Test Set Assessment\n\n`tidymodels` is _very_ punny, so a bit hard to tell which step is which...\n\n## Acquire Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels); library(readr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ broom        1.0.8     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.3.0     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ nlme::collapse()  masks dplyr::collapse()\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nhotels <- \n  read_csv(\"https://tidymodels.org/start/case-study/hotels.csv\") |>\n  mutate(across(where(is.character), as.factor))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 50000 Columns: 23\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (11): hotel, children, meal, country, market_segment, distribution_chan...\ndbl  (11): lead_time, stays_in_weekend_nights, stays_in_week_nights, adults,...\ndate  (1): arrival_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(hotels)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 50,000\nColumns: 23\n$ hotel                          <fct> City_Hotel, City_Hotel, Resort_Hotel, R…\n$ lead_time                      <dbl> 217, 2, 95, 143, 136, 67, 47, 56, 80, 6…\n$ stays_in_weekend_nights        <dbl> 1, 0, 2, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1, …\n$ stays_in_week_nights           <dbl> 3, 1, 5, 6, 4, 2, 2, 3, 4, 2, 2, 1, 2, …\n$ adults                         <dbl> 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, …\n$ children                       <fct> none, none, none, none, none, none, chi…\n$ meal                           <fct> BB, BB, BB, HB, HB, SC, BB, BB, BB, BB,…\n$ country                        <fct> DEU, PRT, GBR, ROU, PRT, GBR, ESP, ESP,…\n$ market_segment                 <fct> Offline_TA/TO, Direct, Online_TA, Onlin…\n$ distribution_channel           <fct> TA/TO, Direct, TA/TO, TA/TO, Direct, TA…\n$ is_repeated_guest              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ previous_cancellations         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ previous_bookings_not_canceled <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ reserved_room_type             <fct> A, D, A, A, F, A, C, B, D, A, A, D, A, …\n$ assigned_room_type             <fct> A, K, A, A, F, A, C, A, D, A, D, D, A, …\n$ booking_changes                <dbl> 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ deposit_type                   <fct> No_Deposit, No_Deposit, No_Deposit, No_…\n$ days_in_waiting_list           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ customer_type                  <fct> Transient-Party, Transient, Transient, …\n$ average_daily_rate             <dbl> 80.75, 170.00, 8.00, 81.00, 157.60, 49.…\n$ required_car_parking_spaces    <fct> none, none, none, none, none, none, non…\n$ total_of_special_requests      <dbl> 1, 3, 2, 1, 4, 1, 1, 1, 1, 1, 0, 1, 0, …\n$ arrival_date                   <date> 2016-09-01, 2017-08-25, 2016-11-19, 20…\n```\n\n\n:::\n:::\n\n\n\n\n## Initial Split\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Stratified sampling to ensure balance\nsplits      <- initial_split(hotels, \n                             strata = children)\n\nhotel_train <- training(splits)\nhotel_test  <- testing(splits)\n\nhotel_train\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 37,500 × 23\n   hotel   lead_time stays_in_weekend_nig…¹ stays_in_week_nights adults children\n   <fct>       <dbl>                  <dbl>                <dbl>  <dbl> <fct>   \n 1 City_H…       217                      1                    3      2 none    \n 2 City_H…         2                      0                    1      2 none    \n 3 Resort…        95                      2                    5      2 none    \n 4 Resort…       136                      1                    4      2 none    \n 5 City_H…        67                      2                    2      2 none    \n 6 City_H…        56                      0                    3      0 children\n 7 City_H…       130                      1                    2      2 none    \n 8 City_H…        27                      0                    1      1 none    \n 9 Resort…        46                      0                    2      2 none    \n10 City_H…       297                      1                    1      2 none    \n# ℹ 37,490 more rows\n# ℹ abbreviated name: ¹​stays_in_weekend_nights\n# ℹ 17 more variables: meal <fct>, country <fct>, market_segment <fct>,\n#   distribution_channel <fct>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>, previous_bookings_not_canceled <dbl>,\n#   reserved_room_type <fct>, assigned_room_type <fct>, booking_changes <dbl>,\n#   deposit_type <fct>, days_in_waiting_list <dbl>, customer_type <fct>, …\n```\n\n\n:::\n:::\n\n\n\n\n## Pre-Process {.scrollable}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(recipes)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nholidays <- c(\"AllSouls\", \"AshWednesday\", \"ChristmasEve\", \"Easter\", \n              \"ChristmasDay\", \"GoodFriday\", \"NewYearsDay\", \"PalmSunday\")\n\nlr_recipe <- \n  recipe(children ~ ., data = hotels) |> \n  step_date(arrival_date) |> \n  step_holiday(arrival_date, holidays = holidays) |> \n  step_rm(arrival_date) |> \n  step_dummy(all_nominal_predictors()) |> \n  step_zv(all_predictors()) |> \n  step_normalize(all_predictors())\n\nlr_recipe\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:    1\npredictor: 22\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Date features from: arrival_date\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Holiday features from: arrival_date\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Variables removed: arrival_date\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Dummy variables from: all_nominal_predictors()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Zero variance filter on: all_predictors()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Centering and scaling for: all_predictors()\n```\n\n\n:::\n:::\n\n\n\n\n## Fit Models\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_model <- \n  logistic_reg(penalty = tune(), mixture = 1) |> \n  set_engine(\"glmnet\")\n\nlr_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n\n\n:::\n:::\n\n\n\n\n## Select Best\n\nFind a _grid_ of parameters \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_reg_grid <- data.frame(penalty = 10^seq(-4, -1, length.out = 30))\n```\n:::\n\n\n\n\nPerform CV splits: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_folds <- vfold_cv(hotel_train, v = 5)\n```\n:::\n\n\n\n\n## Select Best\n\nDefine a *workflow*: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_workflow <-  \n  workflow() |> \n  add_model(lr_model) |> \n  add_recipe(lr_recipe)\n```\n:::\n\n\n\n\nFit workflow to a grid of parameters: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_results <- \n  lr_workflow |> \n  tune_grid(grid = lr_reg_grid,\n            control = control_grid(save_pred = TRUE, \n                                   save_workflow=TRUE),\n            resamples = lr_folds,\n            metrics = metric_set(roc_auc))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nℹ The workflow being saved contains a recipe, which is 6.79 Mb in ℹ memory. If\nthis was not intentional, please set the control setting ℹ `save_workflow =\nFALSE`.\n```\n\n\n:::\n:::\n\n\n\n\n## Select Best\n\nVisual examination\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_results |> \n  collect_metrics() |> \n  ggplot(aes(x = penalty, y = mean)) + \n  geom_point() + \n  geom_line() + \n  ylab(\"Area under the ROC Curve\") +\n  scale_x_log10(labels = scales::label_number())\n```\n\n::: {.cell-output-display}\n![](09_modeling_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\n## Select Best\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_results |> show_best()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in show_best(lr_results): No value of `metric` was given; \"roc_auc\"\nwill be used.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 7\n   penalty .metric .estimator  mean     n std_err .config              \n     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1 0.00108  roc_auc binary     0.877     5 0.00492 Preprocessor1_Model11\n2 0.00137  roc_auc binary     0.877     5 0.00488 Preprocessor1_Model12\n3 0.000853 roc_auc binary     0.877     5 0.00498 Preprocessor1_Model10\n4 0.00174  roc_auc binary     0.876     5 0.00485 Preprocessor1_Model13\n5 0.000672 roc_auc binary     0.876     5 0.00503 Preprocessor1_Model09\n```\n\n\n:::\n\n```{.r .cell-code}\nlr_best <- lr_results |> select_best()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in select_best(lr_results): No value of `metric` was given; \"roc_auc\"\nwill be used.\n```\n\n\n:::\n\n```{.r .cell-code}\nlr_best\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1 0.00108 Preprocessor1_Model11\n```\n\n\n:::\n:::\n\n\n\n\n\n## Refit Best Model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_best_fit <- lr_results |> fit_best()\nlr_best_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_date()\n• step_holiday()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"binomial\",      alpha = ~1) \n\n    Df  %Dev   Lambda\n1    0  0.00 0.078450\n2    2  2.46 0.071480\n3    2  5.20 0.065130\n4    3  7.76 0.059350\n5    4 10.20 0.054070\n6    4 12.46 0.049270\n7    4 14.12 0.044890\n8    5 15.61 0.040900\n9    5 17.01 0.037270\n10   5 18.09 0.033960\n11   5 18.96 0.030940\n12   6 19.95 0.028190\n13   6 20.80 0.025690\n14   6 21.51 0.023410\n15   8 22.40 0.021330\n16   8 23.12 0.019430\n17   8 23.69 0.017710\n18   9 24.18 0.016130\n19  10 24.70 0.014700\n20  11 25.20 0.013390\n21  11 25.63 0.012200\n22  11 25.99 0.011120\n23  13 26.33 0.010130\n24  14 26.65 0.009232\n25  17 26.98 0.008412\n26  19 27.36 0.007665\n27  23 27.78 0.006984\n28  25 28.30 0.006363\n29  28 28.78 0.005798\n30  30 29.23 0.005283\n31  32 29.66 0.004814\n32  35 30.16 0.004386\n33  40 30.61 0.003996\n34  40 31.03 0.003641\n35  43 31.40 0.003318\n36  48 31.74 0.003023\n37  51 32.06 0.002755\n38  54 32.32 0.002510\n39  61 32.56 0.002287\n40  63 32.78 0.002084\n41  67 32.98 0.001899\n42  74 33.15 0.001730\n43  80 33.32 0.001576\n44  85 33.46 0.001436\n45  87 33.58 0.001309\n46  90 33.69 0.001192\n\n...\nand 38 more lines.\n```\n\n\n:::\n:::\n\n\n\n\n## Test Set Assessment\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(lr_best_fit, hotel_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12,500 × 1\n   .pred_class\n   <fct>      \n 1 none       \n 2 children   \n 3 none       \n 4 none       \n 5 none       \n 6 none       \n 7 none       \n 8 none       \n 9 none       \n10 none       \n# ℹ 12,490 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(lr_best_fit, hotel_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12,500 × 26\n   .pred_class .pred_children .pred_none hotel  lead_time stays_in_weekend_nig…¹\n   <fct>                <dbl>      <dbl> <fct>      <dbl>                  <dbl>\n 1 none               0.0343      0.966  Resor…       143                      2\n 2 children           0.942       0.0583 Resor…        47                      0\n 3 none               0.0478      0.952  City_…        80                      0\n 4 none               0.128       0.872  City_…         6                      2\n 5 none               0.00235     0.998  Resor…        16                      1\n 6 none               0.0178      0.982  Resor…       209                      2\n 7 none               0.0208      0.979  City_…         1                      1\n 8 none               0.0242      0.976  City_…        43                      1\n 9 none               0.0201      0.980  City_…         0                      0\n10 none               0.0591      0.941  Resor…       171                      2\n# ℹ 12,490 more rows\n# ℹ abbreviated name: ¹​stays_in_weekend_nights\n# ℹ 20 more variables: stays_in_week_nights <dbl>, adults <dbl>,\n#   children <fct>, meal <fct>, country <fct>, market_segment <fct>,\n#   distribution_channel <fct>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>, previous_bookings_not_canceled <dbl>,\n#   reserved_room_type <fct>, assigned_room_type <fct>, …\n```\n\n\n:::\n:::\n\n\n\n\n\n## Other `tidymodels` tools\n\n- Feature Importance Scores\n- Model Stacking\n- Probabilistic Predictions\n- Uncertainty Bounds (Conformal Inference)\n- Multilevel (Mixed-Effect) Models\n- Fairness Audits\n\n\n## Looking Ahead\n\n\nApplications to course project: \n\n::: {.incremental}\n\n- Two variables are *statistically independent* if\n  one cannot be used to predict the other\n- Many questions take the form \"Is $X$ related to $Y$?\"\n- Build a *model* to predict $Y$ from $X$\n  - If model doesn't use $X$ (small coefficient or importance), unlikely to be related\n  - Move beyond simple linear correlation\n- Also consider *occlusion* studies\n\n:::\n\n## Learning More\n\nFor more: \n\n- Modeling - future STA and CIS classes\n- `tidymodels` - [Case Study](https://www.tidymodels.org/start/case-study/)\n\n\n. . . \n\nTo see more worked examples of predictive models, check out the \n[`tidymodels` Gallery](https://www.tidymodels.org/learn/)\n\n\n",
    "supporting": [
      "09_modeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}